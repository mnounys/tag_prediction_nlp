{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "670292cc-722b-458a-a01c-5a9ba1daf000",
   "metadata": {},
   "source": [
    "# Objectif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37d5663-0f48-4194-aa6a-e1c42a778cc7",
   "metadata": {},
   "source": [
    "Le traitement de langage naturel plus connu sous le nom NLP pour Natural Language Processing, est un domaine dans lequel l'intelligence artificielle essaie de comprendre et donner du sens aux contenus textuelles. L'ojectif de cet article est de s'arrêter sur quelques notions clés du domaine. Le but est faire comprendre l'intuition générale de ces modèles tant prisées, qui font tant parler d'eux et dont le domaine d'application peut être vaste et varié."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f103d97b-23f3-4eb9-a0c7-3bf5cb7b7794",
   "metadata": {},
   "source": [
    "En utilisant le NLP, les développeurs peuvent organiser et structurer les connaissances pour effectuer des tâches linguistiques telles que : la synthèse/le résumé automatique de contenus textuels, la traduction d’une langue vers une autre, l'analyse des sentiments (message positif, message à caractère sensible, …), la reconnaissance vocale ou encore la segmentation des sujets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc445e7-db3d-4963-9eec-beebf416f7d9",
   "metadata": {},
   "source": [
    "Le NLP est un domaine applicatif de l’IA assez complexe parce que comprendre le langage humain, c'est comprendre non seulement les mots, mais aussi les concepts sous-jacents et la façon dont, liés entre eux, des mots assez proches peuvent former des phrases complètement différentes. Dans ce document nous allons essayer de présenter les différentes techniques existentes pour le traitement de texte, et de simplifier les notions complexes utilisées par ces technique. On finira bien évidemment par présenter le modèle qui a permis d'améliorer considérablement les avancées sur le domaine. Ce modèle n'est autre que le fameux BERT introduit par Google en 2018. Une fois la partie théorique abordée on montrera une petite application de ce modèle et comment on peut l'appliquer facilement sur un jeu de donnée textuel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73295a81-0cec-4c15-b209-a6f77248c1ba",
   "metadata": {},
   "source": [
    "# Contexte et motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c20204-687b-47a1-b1de-a42e22c48794",
   "metadata": {},
   "source": [
    "Dans un des précédents projets nous avons pu aider la communauté Stack Overflow pour leur fournir un module qui pemet de proposer des tags grâce à une analyse des mots utilisées dans la question. Pour ce faire nous avons utilisé des méthodes de modélisation qui peuvent être décrite comme étant des méthode historiques. Cette méthode consiste briévement à restructurer le texte de tel sorte à supprimer les mots de la langue qui n'apporte pas beaucoup de sens dans la phrase. Souvent utilisé comme lien de coordination, ces termes ne permet que de lier les pensées ou phrases entre elles. Autre méthode utilsée il s'agit de ramener les différent à leur racine. En combinant ces méthodes nous pouvons alors comptabiliser le nombre d'occurence des mots et ainsi essayer de deviner le contexte générale du texte. On voit d'ores et déjà que cette approche a l'inconvénient de prendre chaque mot indifféremment du contexte dans lequel il se trouve. Cela veut dire qu'un mot qui a plusieurs sens différent serait comptabilisé de la même façon peu importe son contexte. Aussi plus l'utilisation grammaticale est dense et variée plus l'analyse devient plus compliquée."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9dd8ec-745f-4492-bef5-20942a9dc05a",
   "metadata": {},
   "source": [
    "Il va de soi que cette méthode et approche a permis de faire un grand pas pour pourvoir analyser et traiter du texte grâce à une intelligence artificielle, cependant elle manque un peu de bon sens et de mise en contexte du mot. Il manque la compréhension de la construction de la phrase et aussi peut souvent mal interprété le sens du texte car l'analyse exclus le contexte du mot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07559344-2dc2-4caf-8f83-9947c9bd24a7",
   "metadata": {},
   "source": [
    "L'approche qu'on souhaite aborder est l'utilisation des nouvelles méthodes de réseaux de neuronnes, transfert learning vu dans le dernier projet et ainsi profiter du modèle pré-entrainé par Google qui est devenu depuis sa publication la base de référence de plusieurs avancées dans le domaine de NLP. Il serait d'autant dommage de finir la formation OpenClassroom sans pouvoir tester et appliquer les connaissances acquises avec ce modèle BERT qui fait tant parler de lui."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ac3df4-2f83-4855-bbfd-6855f3480e91",
   "metadata": {},
   "source": [
    "## Liens bibliographique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17198ea0-4318-4c9c-9bba-2da89179144d",
   "metadata": {},
   "source": [
    "Pour pouvoir mettre au point ce projet et cette analyse on s'est inspiré de plusieurs articles, sites et tutoriels. Je vous liste ci-dessous les différentes références utilisées. \n",
    "- Jacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova, 2019, BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding https://arxiv.org/pdf/1810.04805.pdf\n",
    "- Anna Rogers, Olga Kovaleva, Anna Rumshisky, 2020, A Primer in BERTology: What we know about how BERT works https://arxiv.org/pdf/2002.12327v1.pdf\n",
    "- Vaswani et al. Attention is all you need (2017) https://arxiv.org/pdf/1706.03762.pdf\n",
    "- Ajay Halthor, Transformer Neural Networks - EXPLAINED!, 2020, https://www.youtube.com/watch?v=TQQlZhbC5ps\n",
    "- Ajay Halthor, BERT Neural Network - EXPLAINED!, 2020, https://www.youtube.com/watch?v=xI0HHN5XKDo\n",
    "- Chris Manning Richard Socher,  Stanford University School of Engineering, Lecture Collection | Natural Language Processing with Deep Learning (Winter 2017), https://www.youtube.com/playlist?list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6\n",
    "- Prasad Nageshkar, Multi-label Text Classification with BERT and PyTorch Lightning, 2021, https://curiousily.com/posts/multi-label-text-classification-with-bert-and-pytorch-lightning/ Jacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova, 2019, BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding https://arxiv.org/pdf/1810.04805.pdf\n",
    "- Anna Rogers, Olga Kovaleva, Anna Rumshisky, 2020, A Primer in BERTology: What we know about how BERT works https://arxiv.org/pdf/2002.12327v1.pdf\n",
    "- Vaswani et al. Attention is all you need (2017) https://arxiv.org/pdf/1706.03762.pdf\n",
    "- Ajay Halthor, Transformer Neural Networks - EXPLAINED!, 2020, https://www.youtube.com/watch?v=TQQlZhbC5ps\n",
    "- Ajay Halthor, BERT Neural Network - EXPLAINED!, 2020, https://www.youtube.com/watch?v=xI0HHN5XKDo\n",
    "- Chris Manning Richard Socher,  Stanford University School of Engineering, Lecture Collection | Natural Language Processing with Deep Learning (Winter 2017), https://www.youtube.com/playlist?list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6\n",
    "- Prasad Nageshkar, Multi-label Text Classification with BERT and PyTorch Lightning, 2021, https://curiousily.com/posts/multi-label-text-classification-with-bert-and-pytorch-lightning/\n",
    "- Michael PhiMichael Phi, Illustrated Guide to LSTM’s and GRU’s: A step by step explanationIllustrated Guide to LSTM’s and GRU’s: A step by step explanation, 2018, https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21\n",
    "- Christopher Olah, Understanding LSTM NetworksUnderstanding LSTM Networks, 2015, http://colah.github.io/posts/2015-08-Understanding-LSTMs/http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "- (Martin et al., 2019) Martin, L., Muller, B., Suárez, P.J.O., Dupont, Y., Romary, L., de La Clergerie, É.V., Seddah, D. and Sagot, B., 2019. Camembert: a tasty french language model. arXiv preprint arXiv:1911.03894.(Martin et al., 2019) Martin, L., Muller, B., Suárez, P.J.O., Dupont, Y., Romary, L., de La Clergerie, É.V., Seddah, D. and Sagot, B., 2019. Camembert: a tasty french language model. arXiv preprint arXiv:1911.03894."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e759ae-150c-4d48-8940-c246abbdcd3c",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da43e526-64ba-4dd9-933b-c006dcec6e25",
   "metadata": {},
   "source": [
    "Le word embedding est une technique qui permet de modéliser chaque mot d'un dictionnaire sous la forme d'un vecteur à valeurs numériques réelles. L'idée est que tous les mots ayant un sens similaire aient des vecteurs très proche. A l'opposé à la méthode one hot encoding qui consiste à donner une seule valeur numérique positive dans un vecteur dont la dimension est égale au nombre de mot du dictionnaire, cette méthode permet de considérer des espaces vectoriels de dimension bien inférieure au nombre de mots du dictionnaire et d’éviter ainsi les problèmes liés au “fléau de la dimension”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daae6d6b-1988-4c1b-97d0-e550a30e4e87",
   "metadata": {},
   "source": [
    "Dans l’idéal, nous voulons construire un embedding permettant de réaliser dans l’espace de représentation vectorielle le genre de relation suivante : « king – man + woman ≈ queen », à savoir que la représentation du mot « king » moins celle du mot « man » plus celle du mot « woman » permet d’obtenir un vecteur proche de celui de « queen » (Figure “Word Embedding”), ou encore Paris - France + Pologne = Varsovie. L’idée étant de montrer que non seulement les mots proches thématiquement les uns des autres, le seront également une fois projetés dans l’espace vectoriel considéré, mais aussi que des relations plus complexes entre différents concepts sont également captés."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a98681-10be-4434-8e14-d7ef957ad2fb",
   "metadata": {},
   "source": [
    "<center><img src=\"blog-17-3-1.jpg\"/></center>\n",
    "<center>Word embeddings</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc95ffbb-9c21-4eac-9eab-bee717b4b508",
   "metadata": {},
   "source": [
    "Généralement de telles représentations sont construites via l’entraînement d’un réseau de neurones sur des tâches de prédiction. Concrètement un apprentissage est effectué, sur des corpus gigantesques de textes via ces réseaux, soit pour prédire un mot en fonction du contexte (skip-gram), soit pour prédire le contexte en fonction du mot (bag of words). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9bf811-2f3e-443f-9dff-15ed2fba278f",
   "metadata": {},
   "source": [
    "Il est évident que l'intérêt de passer sur une structure vectorielle permet de faciliter ensuite l'application des méthodes connues de Deep Learning. Il n'en reste pas moins qu'on peut toujours se poser la question de quelles combinaison de mots faut il essayer de modéliser et/ou quelle sous partie du mot. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8774f16-ff56-4032-9a20-0dd9e5cf92ab",
   "metadata": {},
   "source": [
    "# N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd2f47b-6df7-43db-b717-41915005a6e5",
   "metadata": {},
   "source": [
    "Dans la langue française tout aussi bien que la langue anglaise, on peut avoir des compositions différentes d'une même racine de mot avec des suffixes et/ou préfixes. Généralement le préfixe vient à l'opposé de la racine là ou le suffixe ne change pas beaucoup le sens du mot. Un embedding pertinent dans ce cas de figure serait de représenter les deux mots opposés dans deux sens différent là ou les mots similaire dans la même direction. La considération du N-gram est une technique qui permettrait de faciliter la modélisation de ces mots. \n",
    "\n",
    "À titre d'exemple, le bi-gramme le plus fréquent de la langue française est « de », comme dans l'article « de », mais aussi comme dans les mots « demain », « monde » ou « moderne ». En traitement du langage naturel il est plus fréquent de parler de N-gramme pour désigner des séquences de mots et non de lettres. \n",
    "\n",
    "C’est le type de modélisation proposée par fastText pour la constitution de ses embeddings. L’idée est de considérer chaque mot comme l’ensemble des n-grams qui le constitue, plus le mot lui-même. Par exemple, pour n=3, le mot « where » est constitué des éléments suivants « _wh », « whe », « her », « ere », « re_ » et « where ».  L’embedding du mot correspond alors à la somme des tous les vecteurs associés à l’ensemble des n-grams qui le constituent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9d854e-2975-423c-ace9-1240e54226a3",
   "metadata": {},
   "source": [
    "Regardons un peu comment se déroule les n-grammes appliqué a des séquences de mots. Prenons les exemples suivants :\n",
    "- San francisco\n",
    "- Les trois mousquetaires\n",
    "- Il est en retard\n",
    "Bien évidemment il est plus fréquent de voir San francisco et les trois mousquetaires que la séquence il est en retard. la dernière séquence de mot est un exemple de N-grammes pas très fréquent. L'idée est donc d'appliquer de voir la même succession de mot. Cela permet d'aider à definir quelle combinaison de mot peut être commme un seul mot, ou encode de faciliter la prédiction du prochain mot. Autre application peut être aussi de corriger les erreurs de frappes. Par exemple si on sait que \"café\" à de forte probabilité d'apparaître après la séquence \"boire du ...\", on peut corriger la faute \"caffé\" en faisant une étude de similitude basé sur les lettres. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05523a46-1b86-4d58-bee0-80d006194842",
   "metadata": {},
   "source": [
    "# Text embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4338c4-e0cd-4313-a08c-39771d1e4ceb",
   "metadata": {},
   "source": [
    "Traitons le cas du texte embedding. Souvent il est plus pertinent de considérer plus de mot, notamment pour l'exercice de classification de texte ou de la génération d'un résumé. Il existe alors plusieurs façons de procéder :\n",
    "- faire la moyenne de l’embedding associé aux mots de la phrase ;\n",
    "- effectuer une moyenne pondérée (par exemple avec les poids calculés via procédure TF-IDF, une mesure statistique caractérisant l’importance, en nombre d’occurrences, du mot dans un texte ou un corpus donné).\n",
    "\n",
    "De telles approches sont possibles mais ne sont pas toujours les mieux adaptées. En effet, elles reviennent à utiliser a posteriori un embedding de mots qui a été entraîné dans un contexte différent de celui pour lequel l’embedding de phrase est recherché. Il devient donc important de bien choisir son modèle d'embedding selon l'application qu'on souhaite en faire.\n",
    "\n",
    "Une fois le concept des embeddings bien assimilé, nous pouvons désormais nous tourner vers les autres grands concepts utilisés dans le cadre d’une approche deep learning et voir le cheminement menant de ces concepts à l’état de l’art actuel du NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a40419-fec5-4d16-803c-eef06877e8b3",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a465e086-84ec-473c-9269-8b65607e6a78",
   "metadata": {},
   "source": [
    "Un réseau de neurones récurrents (RNN) est un type de réseau de neurones artificiels qui utilise des données séquentielles ou des données de séries chronologiques. Ce type d’algorithme d'apprentissage profond est couramment utilisé pour la résolution de problèmes temporels, tels que la traduction, la reconnaissance vocale et le sous-titrage d'images. On peut retrouver cet algorithme dans des applications populaires comme Siri ou Google Translate. \n",
    "\n",
    "À l'instar des réseaux de neurones à propagation avant (Feedforward) et convolutifs (CNN), les réseaux de neurones récurrents utilisent des données d'entraînement pour apprendre. Ils se distinguent par leur « mémoire » car ils prennent des informations d'entrées antérieures pour influencer l'entrée et la sortie actuelles. Alors que les réseaux de neurones profonds traditionnels supposent que les entrées et les sorties sont indépendantes les unes des autres, la sortie des réseaux de neurones récurrents dépend des éléments antérieurs de la séquence. Les événements futurs sont également utiles pour déterminer la sortie d'une séquence donnée mais les réseaux de neurones récurrents ne peuvent hélas pas tenir compte de ces événements dans leurs prédictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95265ba-8f30-4ea2-adf0-57f77bd12fc4",
   "metadata": {},
   "source": [
    "Les RNN souffre d'une mémoire très courte. Si une séquence est trop longue, ils ont du mal à propager l'information apprise plus tôt. Ainsi si on essaie de traiter un paragraphe pour effectuer des prédictions, RNN pourrait avoir oublié des informations très importantes lues au début. Autrement dit, si l'état précédent qui influence la prédiction actuelle n'est pas dans un passé proche, le modèle RNN peut ne pas être en mesure de prédire avec précision l'état actuel du système. \n",
    "\n",
    "Le réseau souffre du problème dit \"Vanishing Gradients\". Les gradients sont les valeurs qui permettent de mettre à jour les poids d'un réseau de neuronne. Le problème \"Vanishing Gradients\" est observé quand le gradient devient trop petit en se propageant dans le temps. Si le gradient devient trop petit il ne contribue plus trop à l'apprentissage. Dans l'architecture RNN les neuronnes qui reçoivent un petit gradient arrête d'apprendre, ainsi ils peuvent aussi oublier ce qui a été vu dans des longues séquences et pour cela avoir une mémoire très courte. Pour y remédier, la typologie des LSTM est spécifique avec des « cellules » dans les couches cachées du réseau de neurones. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc19d6f2-b0d4-497b-87f6-fc83c43a251b",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ad9f4a-c6f0-496d-9c78-e5397ee70443",
   "metadata": {},
   "source": [
    "Le principe du LSTM (Long Short Term Memory) est de garder la trace de l’influence entre les différents éléments d’une séquence au contraire des architectures neuronales classiques. Dans le contexte d’une analyse de texte, il apparaît naturel de déterminer le sens d’un mot à partir de ceux qui le précèdent (ou qui le suivent). En pratique, la prise en compte de telles dépendances sur le long terme n’est pas toujours effectuée de façon efficace par les RNN (Recurrent Neural Networks) classiques. C’est la raison pour laquelle le principe des LSTM a été développé en 1997.\n",
    "\n",
    "Considérons par exemple la phrase “J’ai grandi en France… Je parle couramment français”. Si nous voulons détecter le dernier mot de la phrase (français) le contexte proche indique qu’il s’agit vraisemblablement d’une langue. Néanmoins pour établir le contexte et deviner laquelle dont il s’agit il faut remonter plus loin jusqu’au mot France. Cette dépendance à long terme ne peut généralement pas être établie par les RNN classiques et nécessite alors l’utilisation des LSTM.\n",
    "\n",
    "Le principe classique d’une architecture LSTM est d’être composée d’un ensemble de “cellules”. Chacune de ces cellules (variantes mises à part) est composée de trois portes :\n",
    "- une porte d’entrée (input gate) ;\n",
    "- une porte de sortie (output gate) ;\n",
    "- une porte d’oubli (forget gate).\n",
    "\n",
    "La porte d’entrée caractérise à quel point la nouvelle valeur courante doit modifier le contenu de la cellule ou non. La porte d’oubli caractérise le niveau d’information de la cellule qui sera ou non conservé par la suite. La porte de sortie sert à caractériser à quel point la valeur de la cellule participe à l’activation de la cellule.\n",
    "\n",
    "<center><img src=\"1_0f8r3Vd-i4ueYND1CUrhMA.png\"/></center>\n",
    "<center>Structure d'un cellule LTSM</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2118619-96db-474f-aa9e-8575ab25cc37",
   "metadata": {},
   "source": [
    "Regardons un peu de plus prêt le schéma ci-dessus et essayons de détailler un peu plus le concept LTSM. Le sigmoid est une fonction permettant d'avoir une sortie a valeur positive ou nulle. Le tanh est une fonction permettant de normalisé la sortie pour garder un état avec des valeurs entre -1 et 1. \n",
    "\n",
    "Le mécanisme des trois portes va essayer de simuler la fonction de mémoire de la cellule en n'essayant de garder que des informations jugées utiles. Dans un premier temps l'information transite par la porte d'oubli qui peut influencer ou non la remise à 0 de la cellule. La porte d'entrée quant à elle décide si la nouvelle information doit être incorporée à l'état de la cellule. Notre nouvel état de cellule est désormais défini et permettra de déterminer notre sortie. La porte de sortie va déterminer si l'information reçue doit être incluse dans la sortie de la cellule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f840b729-ba8b-4962-87c2-f30d4069a4bf",
   "metadata": {},
   "source": [
    "Une complexification classique de ce modèle en NLP est le Bidirectional LSTM dans laquelle on examine à la fois l’information passée mais aussi l’information future pour analyser une série temporelle. Dans le cadre de l’analyse d’un texte ou d’une phrase, il apparaît naturel que des “indices” concernant la signification d’un mot soient contenus à la fois sur les mots situés avant mais aussi sur les mots situés après. Une application typique est l’analyse de sentiment d’une phrase c’est à dire déterminer si une phrase caractérise un sentiment exprimé plutôt positif ou plutôt négatif.\n",
    "\n",
    "Compte tenu de toutes les connexions nécessaires au pilotage de la cellule mémoire, les couches de neurones de type LSTM sont deux fois plus \"lourdes\" que les couches récurrentes simples, qui elles-mêmes sont deux fois plus lourdes que les couches denses classiques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5956b88a-fa09-47ac-af32-d63b9996874b",
   "metadata": {},
   "source": [
    "# Encoder decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e04973f-0afd-475f-ac2b-032435196d92",
   "metadata": {},
   "source": [
    "L‘Encodeur-Décodeur est un réseau de neurones découvert en 2014 et utilisé dans de nombreux projet. C’est un pilier fondamental dans les logiciels de traduction. Le principe d’un encodage est de transformer la donnée d’entrée en une représentation dans une dimension donnée, puis de la transformer à nouveau dans un espace plus grand ou plus petit qui correspond à la dimension de la sortie attendue. Typiquement la traduction de texte n'est pas quasi littérale, ainsi des mots peuvent être traduits avec deux mots dans une autre langue, selon le contexte et plein d'autres paramètres. Regardons de plus prêts cette architecture. \n",
    "\n",
    "<center><img src=\"blog-17-3-5.jpg\"/></center>\n",
    "\n",
    "Le modèle se base sur 3 composants : l'encodeur, le vecteur encodé et le décodeur.\n",
    "\n",
    "- **L'encodeur** : Généralement un empilement de plusieurs briques RNN (LTSM aussi pour plus de performances) où chacune prend en charge un élément de la séquence d’entrée avant de propager l’information extraite à la suivante\n",
    "- **Le vecteur encodé** : Il correspond à l’état final en sortie de l’encoder. La raison d’être de ce vecteur est de contenir l’information récupérée via la séquence d’entrée en y rajoutant du contexte pour aider le decodeur à en faire en une prédiction pertinente. Il sera l'état intial caché du décodeur.\n",
    "- **Le decodeeu** : Il est constitué d’un empilement de réseaux de neurones récurrents (RNN) où chacun prédit un élément de la séquence de sortie. Chaque réseau accepte un état caché issu du précédent et en envoie un au suivant. La sortie finale est l’ensemble des mots de sortie de chaque réseau du decoder. \n",
    "\n",
    "Les Encodeur-Décodeurs sont largement utilisés dans le monde de la recherche mais ils ont un défaut. Le vecteur que l’Encodeur produit est fixe. Il va être efficace pour des tâches où la phrase est petite mais dès que la phrase est trop grande, le vecteur ne va pas pouvoir stocker toutes l’informations nécessaire. La traduction finale de cette grande phrase ne sera donc pas très pertinente.\n",
    "Les Encodeur-Décodeurs sont aujourd’hui couplés à des méchanismes qui leur permettent de s’adapter à toute longueur de phrase, le mécanisme d'attention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62a7e3f-f404-4c13-bd97-df6963a6e1f8",
   "metadata": {},
   "source": [
    "# Mécanisme d’attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a510384e-e52d-43f5-8f85-a2b214bdc99a",
   "metadata": {},
   "source": [
    "L'article \"Attention Is All You Need\" publié par Google Brain et Google Research en 2017 présentait des techniques pour le calcul parallèle de masse sur Google TPU. L’attention permet au réseau d'effectuer un zoom avant précis et de se concentrer sur les mots contextuels pertinents à la fois dans la séquence d'entrée et dans les sorties prédites.\n",
    "\n",
    "L’Attention est une amélioration des Encodeurs-Décodeurs. Au lieu de se concentrer uniquement sur la sortie finale du RNN, l’Attention va prélever des informations lors de chacune des étapes du RNN (voir schéma ci-dessous) : \n",
    "<center><img src='EncoderVSAttention-1024x417.png'/> </center>\n",
    "On peut voir ci-dessus que dans une couche RNN classique on a une sortie pour chaque mot. Chaque sortie (ou résultat) sera utilisé pour calculer la sortie du mot suivant et ainsi de suite. C’est la récurrence.\n",
    "Dans une couche RNN avec Attention, on a aussi ce calcul récurrent sur chaque mot. Mais en plus de cela, on garde chacune de ces sorties récurrentes en mémoire pour former la sortie finale.\n",
    "\n",
    "Le Décodeur de l’Attention utilise, lui aussi, un RNN mais il ajoute ensuite d’autres calculs plus complexes.Sa fonction principale est de comprendre les relations entre chacun des mots. Cette relation va nous permettre d’identifier les liens qui unissent les mots entre eux. Ainsi notre modèle va comprendre quel verbe se rapporte à quel sujet, quel sujet est associé à quel adjectif, etc. \n",
    "\n",
    "<center> <img src='AttentionViz-768x738.png'/> </center>\n",
    "Sur ce schéma, on peut voir les liens qui unissent les mots de la phrase « L’animal n’a pas traversé la rue car il était trop épuisé » (en anglais) après avoir utilisé le mécanisme de l’Attention. En se focalisant sur le mot « animal », on peut voir qu’il est directement relié à « était » et « épuisé ». Ici, l’Attention nous prouve que même si les mots sont très éloignés dans la phrase, le modèle arrive comprendre leurs relations. C’est tout l’avantage du mécanisme de l’Attention. Lors de l’entraînement des réseaux de neurones, le modèle focalise son attention sur chaque mot de la phrase.\n",
    "Ainsi le modèle peut détecter le contexte des mots et avoir une compréhension globale de la phrase.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a0b300-c39a-4846-8392-4dca13752334",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fba6d16-03bb-4fab-925a-1d7a9f47ce19",
   "metadata": {},
   "source": [
    "Le Transformer est un modèle de deep learning qui se caractérise par le fait qu’il n’a pas besoin de traiter la donnée dans l’ordre, autrement dit, dans le contexte du NLP, n’a pas besoin de traiter le début d’une phrase avant sa fin. Cette caractéristique permet au Transformer une certaine capacité de parallélisation des données (par rapport aux RNN et CNN). Il est constitué de deux composants principaux à savoir une série d’encodeurs suivi d’une série de décodeurs. Le principe des LSTM n’est pas utilisé, ni même celui plus général des réseaux récurrents (RNN). L’enjeu de garder en mémoire l’information contenue par les mots éloigné est donc résolu. À chaque instant l’algorithme a accès, sans perte d’information, à l’ensemble des états successifs parcourus lors de la procédure. Par contre, il s’appuie fortement sur le principe d’Attention. En cela, ils constituent une rupture importante d’avec l’état de l’art précédant leur introduction. L’un des enseignements de cette approche est que le mécanisme d’attention seul, sans traitement séquentiel récurrent, est suffisamment puissant pour faire aussi bien, et mieux, que des RNN avec attention.\n",
    "\n",
    "<center><img src='blog-17-3-7.jpg'/> </center>\n",
    "<center> Architecture de base du Transformer </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668fa0c7-c8c9-4b9f-8cdc-4d6c1592121f",
   "metadata": {},
   "source": [
    "L'image présente plus précisément l'architecture d'un transformer, le côté gauche représente l'encodeur là où la partie à droite concerne le décodeur. Les deux sont composés de plusieurs de plusieurs module pouvant s'empiler plusieurs fois (Nx dans la figure). Chaque module est constitué essentiellement de couche \"Multi-Head Attention\" et \"Feed forward\". Essayons de détailler un peu plus ce modèle de transformer. \n",
    "\n",
    "Prenons l'exemple d'une traduction d'une phrase d'anglais à français, le principe est donc d'encoder la phrase en anglais et la décoder par la suite en français. Comme on le sait très bien les ordinateurs ne comprennent pas les mots, ils aiment traiter des nombres, des vecteurs ou des matrices. De ce fait nous appliquons d'abord une représentation des mots dans un espace d'embedding, ajouté a cela un \"positional encoding\" qui aide à apporter le bon sens au mot selon le contexte. Nous avons alors un vecteur avec le bon contexte qui sera traité par la couche \"Multi-Head Attention\". Cette couche calule plusieurs vecteurs d'attention pour chaque mot de la séquence. Les vecteurs étant indépendant nous pouvons alors les paraleliser avant de combiner les résutltats en appliquant des poids sur les différents vecteurs d'attention. La couche \"Feed Forward\" permet de transformer les vecteurs d'attention dans un format compréhensible par le prochain bloc (Décodeur ou Encodeur).\n",
    "\n",
    "Notre décodeur prend en entrée de façon séquentielle la sortie de notre traduction. De la même façon que notre encodeur il transforme notre mot en français en vecteur et lui ajoute le contexte. Ce vecteur est ensuite traité par une couche \"Masked Multi-Head Attention\", on parle d'une couche masqué car il traite l'attention en ayant des mots masqués. Le résultat notre couche est alors combiné avec le résultat d'attention de notre décodeur avant de re-rentrer dans une couche qu'on peut appeler Décodeur-Encodeur d'attention. C'est dans cette couche que la partie principale de mapping Français Anglais s'effectue. On a en sortie un vecteur d'attention combiné entre Français et Anglais. Ensuite une couche feed forward permet de générer un vecteur interprétable soit par le prochain block de décodeur ou par la couche quui permet de générer une probabilité de distribution qui est humainement interprétable et qui à la fin va pouvoir correspondre à un mot avec la plus grande probabilité.  \n",
    "\n",
    "Ces séquences sont répétées plusieurs fois jusqu'à obtenir la traduction complète de la phrase et voir le token de fin de phrase. Un Transformer amélioré spécialisé pour le NLP, BERT, a été introduit quatre ans plus tard et fait l’objet d’une description ci-dessous."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4e33b9-62f7-45b1-ab29-ad8035e374de",
   "metadata": {},
   "source": [
    "# C'est quoi BERT ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4af72cc-0f71-4398-9844-5f990103c5ba",
   "metadata": {},
   "source": [
    "BERT est un accronyme anglais : **Bidirectional Encoder Representations from Transformers**. Il s'agit d'un modèle créé et publié par google en 2018. En 2019, Google annone avoir commencé à utiliser ce modèle dans ses moteurs de recherche. BERT profite de l'architecture des transfomer en utilisant que la partie gauche du schéma précédent (L'encodeur), couplé à du traitement bi-directionnelle qui lui permet d'améliorer sa compréhension des mots et du contexte. Ce modèle bi-directionnel, est devenu la base de plusieurs recherches dans le domaine de NLP et a permis des grandes avancées."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c47fdf-a4df-4978-bc8f-e133290d2486",
   "metadata": {},
   "source": [
    "Dans une architecture classique d'un transformer, on a deux parties un encodeur et décodeur. Dans un exemple de traduction de texte l'encodeur permet de transformer le texte d'une langue donnée en un vecteur numérique interprétable par la machine qui contient toute l'information englobée par la phrase. Le décodeur quant à lui s'occupe de retransformer ce vecteur numérique dans la langue de traduction. Dans un sens l'encodeur décodeur peuvent être découplé en deux modules ou deux tâches ou l'encodeur permet de comprendre la langue et décodeur d'interpréter la compréhension et la transformer dans un language humainement interprétable. L'encordeur du tranformer est la base de l'architecture du BERT qui permet de comprendre une langue. A l'opposé des autres modèles qui lisait le texte de façon séquentielle, BERT lit toute la séquence en une fois. Cette caractéristique lui donne le titre de bi-directionnel là ou en réalité il n'y a pas de direction. Cette propriété introduite par le principe d'attention permet d'apprendre le contexte en se basant sur les mots qui sont autour (à gauche ou à droite). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d983b451-3b38-494c-ac8f-9d66604fd537",
   "metadata": {},
   "source": [
    "Durant son apprentissage BERT utilise deux méthodes :\n",
    "- MLM : Masked LM. Avant d'utiliser une séquence de mot BERT masque 15% des mots de la séquence, ensuite le modèle essaie de prédire chacun des mots masqués. Il se base pour cela du context fournit par les mots non masqués. BERT apprend le lien entre les mots constituants la même phrase\n",
    "- NSP : Next Sentence Prediction. Le modèle reçoit des paires de phrases en entrée et apprend à prédire si la deuxième phrase est la phrase qui suit dans le texte originale. Avec cette méthode il essaie de comprendre et prédire les liens entre les phrases. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2f7d6d-c068-4f3b-8877-fa338e390dbf",
   "metadata": {},
   "source": [
    "Ainsi les applications sont devenues très simple grâce au BERT. Le modèle pré-entrainé qui a déjà appris la langue peut être utilisé et fine tuné pour répondre à un besoin très précis en rajoutant une petite couche au coeur du modèle. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51db470e-6119-43df-b3e9-94cd34f75229",
   "metadata": {},
   "source": [
    "# Comment utiliser BERT ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1af5a0-e13e-43d4-857f-05030384bd29",
   "metadata": {},
   "source": [
    "Dans un document à part nous allons détailler les étapes que nous allons suivre pour une utilisation du modèle BERT. Pour notre application du modèle de BERT nous allons utiliser Pytorch Lightening qui est un Deep Learning framework permettant de faciliter l'implémentation des modèles Deep Learning. \n",
    "\n",
    "En terme de données nous reprenons le même jeu de données utilisées dans le projet de prédiction des tags des questions Stack Overflow. Nous allons traiter un problème de classification multi label. Autrement dit une question peut appartenir à plusieurs classes, là où un problème multi classe veut simplement dire que notre observation ne peut avoir qu'une seule valeur positve parmi les classes de prédiction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ea13d5-808a-4132-afc5-7a6d6d6eb749",
   "metadata": {},
   "source": [
    "Nous n'allons pas trop s'attarder sur l'analayse de notre jeu de donnée car celle-ci a déjà été effectuée dans le projet précédent. Nous allons suivre les étapes suivantes :\n",
    "- Charger les données et les prétraiter\n",
    "- Préparer les dataset pytorch et lightening DataModule\n",
    "- Définir le modèle\n",
    "- Entrainer le modèle en utilisant les librairies Lightening Trainer\n",
    "- Evaluer la performance du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9027ee-8ad9-4248-952e-86953a673c9b",
   "metadata": {},
   "source": [
    "Le modèle BERT a été pré-entrainé sur plusieurs articles de Wikipédia et de *Book corpus* il a donc une bonne compréhension de la langue anglaise. Cependant nous souhaitons l'appliquer sur des questions issues de Stack OverFlow qui sont plutôt orientés langage de programmation et qui dans bien des cas présente du code issue d'un langage de programmation. Nous allons voir si BERT saurait bien s'adapter à ce domaine qui n'est peut être pas le langage naturel humain.\n",
    "\n",
    "Pour l'analyse du texte français il existe un modèle pré-entrainé qui a été utilisé pour apprendre la langue française. Le modèle porte le nom de **camenBert**. Un petit jeu de mot pour un petit clin d'oeil à la communauté française."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c646c489-684a-4a49-a6b1-c51efc575a95",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e4fea9-962d-4c73-84b6-7687feac2ecd",
   "metadata": {},
   "source": [
    "Nous avons pu atteindre notre objective de compréhension des différentes techniques et méthodes utilisées dans le domaine du NLP. Nous avons montré tous les éléments de modélisation qui précédait l'arrivée du BERT et qui permet tout aussi de comprendre son fonctionnement. Nous avons parlé nottamment : \n",
    "- Des méthodes d'embedding, qui permettent de modéliser des mots en vecteurs\n",
    "- Des réseaux de neuronnes réccurents qui permet de se rappeler d'un état antérieur mais souffre d'une petite mémoire\n",
    "- LTSMs une version améliorée des réseaux de neuronnes réccurent qui souhaitait corriger le problème de mémoire ou vanishing gradient\n",
    "- Encodeur Décodeur qui permet de réutiliser tous ces composants pour traiter du texte dans une dimension avant de le décoder dans une autre dimension\n",
    "- Le principe d'attention introduit avec les transformer qui est une version 2.0 des encodeur décodeurs\n",
    "- Le fameux modèle BERT introduit par Google qui a réussi comprendre l'anglais et qui avec lequel plein de nouveaux projets et documents de recherche ont vu le jour\n",
    "\n",
    "A la fin nous avons montré comment on pouvait mettre en pratique le modèle BERT en s'attaquant à un problème de classification. Il s'agit d'un projet traité auparavant que nous avons souhaité abordé différemment. Dans ce contexte nous avons réussi à améliorer notre modèle de prédiction de tag des question Stack Overflow. BERT est donc facile à utiliser et peut résoudre beaucoup de problèmatique mais demande comme on l'a vu du temps et des ressources en termes de puissance de calcul non négigeable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6625c53-05cf-4227-a4d6-caa93098a3ea",
   "metadata": {},
   "source": [
    "Ce projet requiert des ressources en GPU. Pour cela nous avons décidé d'utiliser le colab notebook qui permet d'utiliser facilement des ressources en CPU, GPU et TPU. La version payante permet d'utiliser des GPU plus puissantes et observer moins de tentative de Google de déconnecter l'environnement de travail.\n",
    "\n",
    "L'apprentissage est un exercice assez gourmand qui requiert plusieurs heures d'apprentissage. C'est pour cela que nous avons fait le choix de réduire le nombre de classe à prédire à 10 classes contre 100 faits dans le projet précédemment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
